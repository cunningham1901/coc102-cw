{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 21COC102 - Advanced AI Systems\n",
    "\n",
    "#### Coursework Submission - Joe Cunningham\n",
    "\n",
    "The purpose of this coursework is to demonstrate different deep neural network architectures on an image classification\n",
    "problem.\n",
    "\n",
    "The dataset used in this project is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), comprising 60000 3x32x32\n",
    "images with 10 classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I have gpu access for training, but models should be compatible with CPU\n",
    "# Hence, check for a GPU:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Until i figure out if there is a Dataset attrib. for this, this will do....\n",
    "NUM_CLASSES = 10\n",
    "CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the CIFAR-10 dataset & create dataloaders. Adapted from the pytorch quickstart guide\n",
    "[\\[1\\]](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_train = datasets.CIFAR10(\n",
    "    root = 'CIFAR-10',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "CIFAR_test = datasets.CIFAR10(\n",
    "    root = 'CIFAR-10',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(CIFAR_train, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(CIFAR_test, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully Connected Neural Networks\n",
    "\n",
    "The first type of neural network to experiment with is the fully connected network. This network comprises several 1-D\n",
    "layers of neurons, where every neuron in one layer is connected to every neuron in the following layer. The following\n",
    "network, `FullyConnectedNetwork`, has an input layer of 3072 neurons, two hidden layers of 512 neurons each, with a\n",
    "final output/classification layer of 10 neurons. An activation function is placed between each layer to introduce\n",
    "non-linearity.\n",
    "\n",
    "Adapted from the pytorch quickstart guide\n",
    "[\\[1\\]](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.name = config['name']\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        layers = config['layer_sizes']\n",
    "        act_func = config['activation_function']\n",
    "        batch_norm = config['batch_norm']\n",
    "\n",
    "        self.linear_stack = nn.Sequential()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.linear_stack.append(nn.Linear(layers[i][0],layers[i][1]))\n",
    "            if batch_norm: self.linear_stack.append(nn.BatchNorm1d(layers[i][1]))\n",
    "            self.linear_stack.append(act_func)\n",
    "        # Add last layer without a following activation function\n",
    "        self.linear_stack.append(nn.Linear(layers[-1][0], layers[-1][1]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_stack(x)\n",
    "        return x\n",
    "\n",
    "fc_net_config = {\n",
    "    'name' : 'FullyConnectedNetwork_1x128',\n",
    "    'layer_sizes' : [(3*32*32,128),(128,128),(128,10)],\n",
    "    'activation_function' : nn.ReLU(),\n",
    "    'batch_norm' : True\n",
    "}\n",
    "fc_net = FullyConnectedNetwork(fc_net_config).to(device)\n",
    "print(fc_net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Setup model evaluation functions\n",
    "def compute_confusion_matrix(pred_classes, gt_classes, num_classes):\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    for i in range(pred_classes.shape[0]):\n",
    "        confusion_matrix[pred_classes[i],gt_classes[i]] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_precision_recall(confusion_matrix, print_output=False):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    class_precision_recall = torch.zeros(num_classes,2)\n",
    "    for i in range(num_classes):\n",
    "        true_pos = confusion_matrix[i,i]\n",
    "        precision = true_pos / confusion_matrix[i,:].sum()\n",
    "        recall = true_pos / confusion_matrix[:,i].sum()\n",
    "        class_precision_recall[i,0] = precision\n",
    "        class_precision_recall[i,1] = recall\n",
    "        if print_output: print(f\"Class: {i}, Precision: {precision*100:.2f}%, Recall: {recall*100:.2f}\")\n",
    "    avg_precision = class_precision_recall[:,0].sum()/num_classes\n",
    "    avg_recall = class_precision_recall[:,1].sum()/num_classes\n",
    "    if print_output: print(f\"Average Precision: {avg_precision*100:.2f}%, Average Recall: {avg_recall*100:.2f}%\")\n",
    "    return class_precision_recall, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "# Setup training/test functions\n",
    "# Adapted from the pytorch quickstart guide\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimiser:torch.optim.Optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0 # For graphing the change in loss over time\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute preds and loss for batch\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds,y)\n",
    "\n",
    "        #Backprop\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "        train_loss += loss * len(X) #Weight the average to account for the last batch\n",
    "\n",
    "        #Progress tracking\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= size\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    # we will compute a confusion matrix to allow for calculation of\n",
    "    confusion_matrix = torch.zeros(NUM_CLASSES, NUM_CLASSES)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y, = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            test_loss += loss_fn(preds, y).item()\n",
    "            pred_classes = preds.argmax(1)\n",
    "            confusion_matrix += compute_confusion_matrix(pred_classes, y, NUM_CLASSES)\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    class_precision_recall, avg_precision, avg_recall = compute_precision_recall(confusion_matrix, print_output=True)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return [test_loss, correct, confusion_matrix, class_precision_recall,avg_precision, avg_recall]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Run the test a training loops for 5 epochs\n",
    "\n",
    "def train_with_early_stopping(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval=2, min_epochs=2):\n",
    "\n",
    "    stop, epoch = False, 1\n",
    "    train_losses = []\n",
    "    test_results = []\n",
    "    test_losses = []\n",
    "    while not stop:\n",
    "        print(f\"---------- Epoch {epoch} ---------\")\n",
    "        train_losses.append(train(train_dataloader, model, loss_fn, optimiser)) #Train\n",
    "        test_result = test(test_dataloader, model, loss_fn) #Test\n",
    "        # Compile test results\n",
    "        test_results.append(test_result)\n",
    "        test_losses.append(test_result[0])\n",
    "\n",
    "        #Save model\n",
    "        if epoch % save_interval == 0:\n",
    "            filename = f\"{model.name}_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), filename)\n",
    "\n",
    "        #check if current test loss is worse than last 2 epochs\n",
    "        if epoch > min_epochs and epoch > 2:\n",
    "            check1 = test_losses[-1] > test_losses[-2]\n",
    "            check2 = test_losses[-1] > test_losses[-3]\n",
    "            if check1 and check2:\n",
    "                stop = True\n",
    "                print(f\"Early stopping triggered at epoch {epoch}. Done!\")\n",
    "        # next epoch\n",
    "        epoch += 1\n",
    "    return train_losses, test_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(fc_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, fc_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Plot the results of a training session using pyplotlib\n",
    "# Currently plotting the following:\n",
    "#  - Training loss vs Test loss\n",
    "#  - Average precision, recall, and accuracy (test set)\n",
    "#  - Precision per class\n",
    "#  - Recall per class\n",
    "def plot_results(train_losses, test_results):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "    # Plot TrainLoss on ax1\n",
    "    xs, ys = [], []\n",
    "    for epoch, loss in enumerate(train_losses):\n",
    "        xs.append(epoch+1)\n",
    "        ys.append(loss)\n",
    "\n",
    "    ax1.plot(xs, ys, label=\"Train Loss\")\n",
    "\n",
    "    # Plot TestLoss on ax1\n",
    "    test_losses = [i[0] for i in test_results]\n",
    "    xs, ys = [], []\n",
    "    for epoch, loss in enumerate(test_losses):\n",
    "        xs.append(epoch+1)\n",
    "        ys.append(loss)\n",
    "    ax1.plot(xs, ys, label=\"Test Loss\")\n",
    "\n",
    "    # Setup ax1 to show test vs train loss\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Test vs. Train Loss\")\n",
    "\n",
    "    # Plot avg precision (ax2)\n",
    "    ys = [i[4]*100 for i in test_results]\n",
    "    ax2.plot(xs, ys, label='Precision')\n",
    "\n",
    "    # Plot avg recall (ax2)\n",
    "    ys = [i[5]*100 for i in test_results]\n",
    "    ax2.plot(xs, ys, label='Recall')\n",
    "\n",
    "    # Setup ax2\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Percentage\")\n",
    "    ax2.set_title(\"Average Precision/Recall\")\n",
    "\n",
    "    # Show per class precision/recall (ax3/4)\n",
    "    precision_recalls = [i[3] for i in test_results]\n",
    "    xs = [i+1 for i in range(len(precision_recalls))]\n",
    "    for idx, class_name in enumerate(CLASS_NAMES):\n",
    "        ys_p, ys_r = [],[]\n",
    "        for p_r in precision_recalls:\n",
    "            ys_p.append(p_r[idx, 0]*100)\n",
    "            ys_r.append(p_r[idx, 1]*100)\n",
    "        # Precision on ax3, recall on ax4\n",
    "        ax3.plot(xs, ys_p, label=class_name)\n",
    "        ax4.plot(xs, ys_r, label=class_name)\n",
    "    # Setup ax3,4\n",
    "    ax3.legend()\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Percentage\")\n",
    "    ax3.set_title(\"Per Class Precision\")\n",
    "    ax4.legend()\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(\"Percentage\")\n",
    "    ax4.set_title(\"Per Class Recall\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(train_losses, test_results)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "The first network is based on AlexNet, but scaled down to a 32x32 input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "JoeNet(\n  (conv1): Conv2d(3, 6, kernel_size=(7, 7), stride=(2, 2))\n  (conv1_bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (conv3_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (conv4_bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv5): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (conv5_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=400, out_features=256, bias=True)\n  (fc1_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=256, out_features=256, bias=True)\n  (fc2_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class JoeNet(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(JoeNet, self).__init__()\n",
    "        self.name = config['name']\n",
    "        self.batch_norm = config['batch_norm']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 7, 2) #32x32 -> 13x13\n",
    "        self.conv1_bn = nn.BatchNorm2d(6)\n",
    "        self.pool1 = nn.MaxPool2d(3, 1, padding=1) #13x13 -> 13x13\n",
    "        self.conv2 = nn.Conv2d(6,16,5,padding=\"same\") #13x13 -> 13x13\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.pool2 = nn.MaxPool2d(3, 1) #13x13 -> 12x12\n",
    "        self.conv3 = nn.Conv2d(16, 24, 3, padding=\"same\")\n",
    "        self.conv3_bn = nn.BatchNorm2d(24)\n",
    "        self.conv4 = nn.Conv2d(24, 24, 3, padding=\"same\")\n",
    "        self.conv4_bn = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(24, 16, 3, padding=\"same\")\n",
    "        self.conv5_bn = nn.BatchNorm2d(16)\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=2)\n",
    "        self.fc1 = nn.Linear(400, 256)\n",
    "        self.fc1_bn = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc2_bn = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.conv1_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.conv2_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.conv3_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.conv4_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.conv5_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.fc1_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.dropout1(x, )\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.fc2_bn(x)) if self.batch_norm else F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "jn_config = {\n",
    "    'name' : 'JoeNet_test',\n",
    "    'batch_norm' : False\n",
    "}\n",
    "\n",
    "j_net = JoeNet(jn_config)\n",
    "j_net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ---------\n",
      "loss: 2.301653  [    0/50000]\n",
      "loss: 2.312239  [ 6400/50000]\n",
      "loss: 2.305768  [12800/50000]\n",
      "loss: 2.293992  [19200/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(j_net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#     for batch, (X, y) in enumerate(test_dataloader):\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#         preds =j_net(X)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#test_results = test(test_dataloader, j_net, loss_fn)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#print(test_results[2])\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m train_losses, test_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_with_early_stopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mtrain_with_early_stopping\u001B[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval, min_epochs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stop:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---------- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#Train\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     test_result \u001B[38;5;241m=\u001B[39m test(test_dataloader, model, loss_fn) \u001B[38;5;66;03m#Test\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Compile test results\u001B[39;00m\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataloader, model, loss_fn, optimiser)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#Backprop\u001B[39;00m\n\u001B[1;32m     38\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(j_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch, (X, y) in enumerate(test_dataloader):\n",
    "#         preds =j_net(X)\n",
    "#         classes = preds.argmax(1)\n",
    "#         print(classes)\n",
    "#         if batch > 3: break\n",
    "\n",
    "#test_results = test(test_dataloader, j_net, loss_fn)\n",
    "#print(test_results[2])\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, j_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}