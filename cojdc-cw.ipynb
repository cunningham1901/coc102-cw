{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 21COC102 - Advanced AI Systems\n",
    "\n",
    "#### Coursework Submission - Joe Cunningham\n",
    "\n",
    "The purpose of this coursework is to demonstrate different deep neural network architectures on an image classification\n",
    "problem.\n",
    "\n",
    "The dataset used in this project is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), comprising 60000 3x32x32\n",
    "images with 10 classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I have gpu access for training, but models should be compatible with CPU\n",
    "# Hence, check for a GPU:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Until i figure out if there is a Dataset attrib. for this, this will do....\n",
    "NUM_CLASSES = 10\n",
    "CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the CIFAR-10 dataset & create dataloaders. Adapted from the pytorch quickstart guide\n",
    "[\\[1\\]](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_train = datasets.CIFAR10(\n",
    "    root = 'CIFAR-10',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "CIFAR_test = datasets.CIFAR10(\n",
    "    root = 'CIFAR-10',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(CIFAR_train, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(CIFAR_test, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully Connected Neural Networks\n",
    "\n",
    "The first type of neural network to experiment with is the fully connected network. This network comprises several 1-D\n",
    "layers of neurons, where every neuron in one layer is connected to every neuron in the following layer. The following\n",
    "network, `FullyConnectedNetwork`, has an input layer of 3072 neurons, two hidden layers of 512 neurons each, with a\n",
    "final output/classification layer of 10 neurons. An activation function is placed between each layer to introduce\n",
    "non-linearity.\n",
    "\n",
    "Adapted from the pytorch quickstart guide\n",
    "[\\[1\\]](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.name = config['name']\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        layers = config['layer_sizes']\n",
    "        act_func = config['activation_function']\n",
    "        batch_norm = config['batch_norm']\n",
    "\n",
    "        self.linear_stack = nn.Sequential()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.linear_stack.append(nn.Linear(layers[i][0],layers[i][1]))\n",
    "            if batch_norm: self.linear_stack.append(nn.BatchNorm1d(layers[i][1]))\n",
    "            self.linear_stack.append(act_func)\n",
    "        # Add last layer without a following activation function\n",
    "        self.linear_stack.append(nn.Linear(layers[-1][0], layers[-1][1]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_stack(x)\n",
    "        return x\n",
    "\n",
    "fc_net_config = {\n",
    "    'name' : 'FullyConnectedNetwork_1x128',\n",
    "    'layer_sizes' : [(3*32*32,128),(128,128),(128,10)],\n",
    "    'activation_function' : nn.ReLU(),\n",
    "    'batch_norm' : True\n",
    "}\n",
    "fc_net = FullyConnectedNetwork(fc_net_config).to(device)\n",
    "print(fc_net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Setup model evaluation functions\n",
    "def compute_confusion_matrix(pred_classes, gt_classes, num_classes):\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    for i in range(pred_classes.shape[0]):\n",
    "        confusion_matrix[pred_classes[i],gt_classes[i]] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_precision_recall(confusion_matrix, print_output=False):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    class_precision_recall = torch.zeros(num_classes,2)\n",
    "    for i in range(num_classes):\n",
    "        true_pos = confusion_matrix[i,i]\n",
    "        precision = true_pos / confusion_matrix[i,:].sum()\n",
    "        recall = true_pos / confusion_matrix[:,i].sum()\n",
    "        class_precision_recall[i,0] = precision\n",
    "        class_precision_recall[i,1] = recall\n",
    "        if print_output: print(f\"Class: {i}, Precision: {precision*100:.2f}%, Recall: {recall*100:.2f}\")\n",
    "    avg_precision = class_precision_recall[:,0].sum()/num_classes\n",
    "    avg_recall = class_precision_recall[:,1].sum()/num_classes\n",
    "    if print_output: print(f\"Average Precision: {avg_precision*100:.2f}%, Average Recall: {avg_recall*100:.2f}%\")\n",
    "    return class_precision_recall, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "# Setup training/test functions\n",
    "# Adapted from the pytorch quickstart guide\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimiser:torch.optim.Optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0 # For graphing the change in loss over time\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y, = X.to(device), y.to(device)\n",
    "        # Compute preds and loss for batch\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds,y)\n",
    "\n",
    "        #Backprop\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "        train_loss += loss * len(X) #Weight the average to account for the last batch\n",
    "\n",
    "        #Progress tracking\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= size\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    # we will compute a confusion matrix to allow for calculation of\n",
    "    confusion_matrix = torch.zeros(NUM_CLASSES, NUM_CLASSES)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y, = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            test_loss += loss_fn(preds, y).item()\n",
    "            pred_classes = preds.argmax(1)\n",
    "            confusion_matrix += compute_confusion_matrix(pred_classes, y, NUM_CLASSES)\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    class_precision_recall, avg_precision, avg_recall = compute_precision_recall(confusion_matrix, print_output=True)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return [test_loss, correct, confusion_matrix, class_precision_recall,avg_precision, avg_recall]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Run the test a training loops for 5 epochs\n",
    "\n",
    "def train_with_early_stopping(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval=2, min_epochs=2):\n",
    "\n",
    "    stop, epoch = False, 1\n",
    "    train_losses = []\n",
    "    test_results = []\n",
    "    test_losses = []\n",
    "    while not stop:\n",
    "        print(f\"---------- Epoch {epoch} ---------\")\n",
    "        train_losses.append(train(train_dataloader, model, loss_fn, optimiser)) #Train\n",
    "        test_result = test(test_dataloader, model, loss_fn) #Test\n",
    "        # Compile test results\n",
    "        test_results.append(test_result)\n",
    "        test_losses.append(test_result[0])\n",
    "\n",
    "        #Save model\n",
    "        if epoch % save_interval == 0:\n",
    "            filename = f\"{model.name}_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), filename)\n",
    "\n",
    "        #check if current test loss is worse than last 2 epochs\n",
    "        if epoch > min_epochs and epoch > 2:\n",
    "            check1 = test_losses[-1] > test_losses[-2]\n",
    "            check2 = test_losses[-1] > test_losses[-3]\n",
    "            if check1 and check2:\n",
    "                stop = True\n",
    "                print(f\"Early stopping triggered at epoch {epoch}. Done!\")\n",
    "        # next epoch\n",
    "        epoch += 1\n",
    "    return train_losses, test_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(fc_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, fc_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Plot the results of a training session using pyplotlib\n",
    "# Currently plotting the following:\n",
    "#  - Training loss vs Test loss\n",
    "#  - Average precision, recall, and accuracy (test set)\n",
    "#  - Precision per class\n",
    "#  - Recall per class\n",
    "def plot_results(train_losses, test_results):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "    # Plot TrainLoss on ax1\n",
    "    xs, ys = [], []\n",
    "    for epoch, loss in enumerate(train_losses):\n",
    "        xs.append(epoch+1)\n",
    "        ys.append(loss)\n",
    "\n",
    "    ax1.plot(xs, ys, label=\"Train Loss\")\n",
    "\n",
    "    # Plot TestLoss on ax1\n",
    "    test_losses = [i[0] for i in test_results]\n",
    "    xs, ys = [], []\n",
    "    for epoch, loss in enumerate(test_losses):\n",
    "        xs.append(epoch+1)\n",
    "        ys.append(loss)\n",
    "    ax1.plot(xs, ys, label=\"Test Loss\")\n",
    "\n",
    "    # Setup ax1 to show test vs train loss\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Test vs. Train Loss\")\n",
    "\n",
    "    # Plot avg precision (ax2)\n",
    "    ys = [i[4]*100 for i in test_results]\n",
    "    ax2.plot(xs, ys, label='Precision')\n",
    "\n",
    "    # Plot avg recall (ax2)\n",
    "    ys = [i[5]*100 for i in test_results]\n",
    "    ax2.plot(xs, ys, label='Recall')\n",
    "\n",
    "    # Setup ax2\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Percentage\")\n",
    "    ax2.set_title(\"Average Precision/Recall\")\n",
    "\n",
    "    # Show per class precision/recall (ax3/4)\n",
    "    precision_recalls = [i[3] for i in test_results]\n",
    "    xs = [i+1 for i in range(len(precision_recalls))]\n",
    "    for idx, class_name in enumerate(CLASS_NAMES):\n",
    "        ys_p, ys_r = [],[]\n",
    "        for p_r in precision_recalls:\n",
    "            ys_p.append(p_r[idx, 0]*100)\n",
    "            ys_r.append(p_r[idx, 1]*100)\n",
    "        # Precision on ax3, recall on ax4\n",
    "        ax3.plot(xs, ys_p, label=class_name)\n",
    "        ax4.plot(xs, ys_r, label=class_name)\n",
    "    # Setup ax3,4\n",
    "    ax3.legend()\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Percentage\")\n",
    "    ax3.set_title(\"Per Class Precision\")\n",
    "    ax4.legend()\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(\"Percentage\")\n",
    "    ax4.set_title(\"Per Class Recall\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(train_losses, test_results)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "The first network is based on AlexNet, but scaled down to a 32x32 input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "JoeNet(\n  (conv_stack): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (9): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (12): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (13): ReLU()\n    (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU()\n    (17): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc_stack): Sequential(\n    (0): Linear(in_features=6400, out_features=4096, bias=True)\n    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class JoeNet(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(JoeNet, self).__init__()\n",
    "        self.name = config['name']\n",
    "        batch_norm = config['batch_norm']\n",
    "\n",
    "        sf = config['small_features']\n",
    "        mf = config['medium_features']\n",
    "        lf = config['large_features']\n",
    "        fcf = config['fullyconnected_features']\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, sf, 7, 2), # 32x32 -> 13x13\n",
    "            nn.BatchNorm2d(sf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 1, padding=1), # 13x13 -> 13x13\n",
    "            nn.Conv2d(sf,mf,5,padding=\"same\"), # 13x13 -> 13x13\n",
    "            nn.BatchNorm2d(mf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 1), # 13x13 -> 12x12\n",
    "            nn.Conv2d(mf, lf, 3, padding=\"same\"), # 12x12\n",
    "            nn.BatchNorm2d(lf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(lf, lf, 3, padding=\"same\"), # 12x12\n",
    "            nn.BatchNorm2d(lf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(lf, mf, 3, padding=\"same\"), # 12x12\n",
    "            nn.BatchNorm2d(mf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2) # 12x12 -> 5x5\n",
    "        )\n",
    "\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(5*5*mf, fcf),\n",
    "            nn.BatchNorm1d(fcf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(fcf, fcf),\n",
    "            nn.BatchNorm1d(fcf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(fcf, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_stack(x)\n",
    "        return x\n",
    "\n",
    "jn_config = {\n",
    "    'name' : 'JoeNet_test',\n",
    "    'batch_norm' : True,\n",
    "    'small_features': 96,\n",
    "    'medium_features': 256,\n",
    "    'large_features': 384,\n",
    "    'fullyconnected_features': 4096\n",
    "}\n",
    "\n",
    "j_net = JoeNet(jn_config)\n",
    "j_net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ---------\n",
      "loss: 2.384124  [    0/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [48]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(j_net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#     for batch, (X, y) in enumerate(test_dataloader):\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#         preds =j_net(X)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#test_results = test(test_dataloader, j_net, loss_fn)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#print(test_results[2])\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m train_losses, test_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_with_early_stopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mtrain_with_early_stopping\u001B[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval, min_epochs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stop:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---------- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#Train\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     test_result \u001B[38;5;241m=\u001B[39m test(test_dataloader, model, loss_fn) \u001B[38;5;66;03m#Test\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Compile test results\u001B[39;00m\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataloader, model, loss_fn, optimiser)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#Backprop\u001B[39;00m\n\u001B[1;32m     38\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(j_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch, (X, y) in enumerate(test_dataloader):\n",
    "#         preds =j_net(X)\n",
    "#         classes = preds.argmax(1)\n",
    "#         print(classes)\n",
    "#         if batch > 3: break\n",
    "\n",
    "#test_results = test(test_dataloader, j_net, loss_fn)\n",
    "#print(test_results[2])\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, j_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This second CNN is based on VGG16, but one max pooling layer has changed to ensure the\n",
    "size is not reduced below 3x3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16ish(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Identity()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): Identity()\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (8): Identity()\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (11): Identity()\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (15): Identity()\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (18): Identity()\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (21): Identity()\n",
      "    (22): ReLU()\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (24): Identity()\n",
      "    (25): ReLU()\n",
      "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (28): Identity()\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (31): Identity()\n",
      "    (32): ReLU()\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (34): Identity()\n",
      "    (35): ReLU()\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (37): Identity()\n",
      "    (38): ReLU()\n",
      "    (39): MaxPool2d(kernel_size=2, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (41): Identity()\n",
      "    (42): ReLU()\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (44): Identity()\n",
      "    (45): ReLU()\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (47): Identity()\n",
      "    (48): ReLU()\n",
      "    (49): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_stack): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (1): Identity()\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): Identity()\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VGG16ish(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(VGG16ish, self).__init__()\n",
    "\n",
    "        self.name = config['name']\n",
    "        batch_norm = config['batch_norm']\n",
    "        \n",
    "        c1f = config['conv1_features']\n",
    "        c2f = config['conv2_features']\n",
    "        c3f = config['conv3_features']\n",
    "        c4f = config['conv4_features']\n",
    "        c5f = config['conv5_features']\n",
    "        fcf = config['fullyconnected_features']\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            # Conv stack 1 - 2 layers c1f features\n",
    "            nn.Conv2d(3,c1f,3,padding='same'), # 32x32\n",
    "            nn.BatchNorm2d(c1f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c1f,c1f,3,padding='same'), # 32x32\n",
    "            nn.BatchNorm2d(c1f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 32x32 -> 16x16\n",
    "            # Conv stack 2 - 2 layers 128 features\n",
    "            nn.Conv2d(c1f,c2f,3,padding='same'), # 16x16\n",
    "            nn.BatchNorm2d(c2f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c2f,c2f,3,padding='same'), # 16x16\n",
    "            nn.BatchNorm2d(c2f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 16x16 -> 8x8\n",
    "            # Conv stack 3 - 3 layers c3f features\n",
    "            nn.Conv2d(c2f,c3f,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(c3f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c3f,c3f,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(c3f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c3f,c3f,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(c3f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c3f,c3f,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(c3f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), #  8x8 -> 4x4\n",
    "            # Conv stack 4 - 3 layers c4f features\n",
    "            nn.Conv2d(c3f,c4f,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(c4f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c4f,c4f,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(c4f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c4f,c4f,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(c4f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c4f,c4f,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(c4f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1, padding=1), #  4x4 -> 4x4\n",
    "            # Conv stack 5 - 3 layers c5f features\n",
    "            nn.Conv2d(c4f,c5f,3,padding='same'),# 4x4\n",
    "            nn.BatchNorm2d(c5f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c5f,c5f,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(c5f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c5f,c5f,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(c5f) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) #2x2x512\n",
    "        )\n",
    "\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(2*2*c5f, fcf),\n",
    "            nn.BatchNorm1d(fcf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fcf, fcf),\n",
    "            nn.BatchNorm1d(fcf) if batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fcf, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_stack(x)\n",
    "        return x\n",
    "\n",
    "vgg_net_config = {\n",
    "    'name' : 'vgg_test',\n",
    "    'batch_norm' : False,\n",
    "    'conv1_features': 64,\n",
    "    'conv2_features': 128,\n",
    "    'conv3_features': 256,\n",
    "    'conv4_features': 512,\n",
    "    'conv5_features': 512,\n",
    "    'fullyconnected_features': 4096\n",
    "}\n",
    "\n",
    "vgg_net = VGG16ish(vgg_net_config)\n",
    "print(vgg_net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ---------\n",
      "loss: 2.301230  [    0/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [44]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m      2\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(vgg_net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, momentum\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m train_losses, test_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_with_early_stopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvgg_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mtrain_with_early_stopping\u001B[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval, min_epochs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stop:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---------- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#Train\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     test_result \u001B[38;5;241m=\u001B[39m test(test_dataloader, model, loss_fn) \u001B[38;5;66;03m#Test\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Compile test results\u001B[39;00m\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataloader, model, loss_fn, optimiser)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#Backprop\u001B[39;00m\n\u001B[1;32m     38\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(vgg_net.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, vgg_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}