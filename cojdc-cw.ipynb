{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 21COC102 - Advanced AI Systems\n",
    "\n",
    "#### Coursework Submission - Joe Cunningham\n",
    "\n",
    "The purpose of this coursework is to demonstrate different deep neural network architectures on an image classification\n",
    "problem.\n",
    "\n",
    "The dataset used in this project is [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), comprising 60000 3x32x32\n",
    "images with 10 classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I have gpu access for training, but models should be compatible with CPU\n",
    "# Hence, check for a GPU:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Until i figure out if there is a Dataset attrib. for this, this will do....\n",
    "NUM_CLASSES = 10\n",
    "CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the CIFAR-10 dataset & create dataloaders. Adapted from the pytorch quickstart guide\n",
    "[\\[1\\]](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_train = datasets.CIFAR10(\n",
    "    root = 'CIFAR-10',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "CIFAR_test = datasets.CIFAR10(\n",
    "    root = 'CIFAR-10',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(CIFAR_train, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(CIFAR_test, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully Connected Neural Networks\n",
    "\n",
    "The first type of neural network to experiment with is the fully connected network. This network comprises several 1-D\n",
    "layers of neurons, where every neuron in one layer is connected to every neuron in the following layer. The following\n",
    "network, `FullyConnectedNetwork`, has an input layer of 3072 neurons, two hidden layers of 512 neurons each, with a\n",
    "final output/classification layer of 10 neurons. An activation function is placed between each layer to introduce\n",
    "non-linearity.\n",
    "\n",
    "Adapted from the pytorch quickstart guide\n",
    "[\\[1\\]](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.name = config['name']\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        layers = config['layer_sizes']\n",
    "        act_func = config['activation_function']\n",
    "        batch_norm = config['batch_norm']\n",
    "\n",
    "        self.linear_stack = nn.Sequential()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.linear_stack.append(nn.Linear(layers[i][0],layers[i][1]))\n",
    "            if batch_norm: self.linear_stack.append(nn.BatchNorm1d(layers[i][1]))\n",
    "            self.linear_stack.append(act_func)\n",
    "        # Add last layer without a following activation function\n",
    "        self.linear_stack.append(nn.Linear(layers[-1][0], layers[-1][1]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_stack(x)\n",
    "        return x\n",
    "\n",
    "fc_net_config = {\n",
    "    'name' : 'FullyConnectedNetwork_1x128',\n",
    "    'layer_sizes' : [(3*32*32,128),(128,128),(128,10)],\n",
    "    'activation_function' : nn.ReLU(),\n",
    "    'batch_norm' : True\n",
    "}\n",
    "fc_net = FullyConnectedNetwork(fc_net_config).to(device)\n",
    "print(fc_net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Setup model evaluation functions\n",
    "def compute_confusion_matrix(pred_classes, gt_classes, num_classes):\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    for i in range(pred_classes.shape[0]):\n",
    "        confusion_matrix[pred_classes[i],gt_classes[i]] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_precision_recall(confusion_matrix, print_output=False):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    class_precision_recall = torch.zeros(num_classes,2)\n",
    "    for i in range(num_classes):\n",
    "        true_pos = confusion_matrix[i,i]\n",
    "        precision = true_pos / confusion_matrix[i,:].sum()\n",
    "        recall = true_pos / confusion_matrix[:,i].sum()\n",
    "        class_precision_recall[i,0] = precision\n",
    "        class_precision_recall[i,1] = recall\n",
    "        if print_output: print(f\"Class: {i}, Precision: {precision*100:.2f}%, Recall: {recall*100:.2f}\")\n",
    "    avg_precision = class_precision_recall[:,0].sum()/num_classes\n",
    "    avg_recall = class_precision_recall[:,1].sum()/num_classes\n",
    "    if print_output: print(f\"Average Precision: {avg_precision*100:.2f}%, Average Recall: {avg_recall*100:.2f}%\")\n",
    "    return class_precision_recall, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "# Setup training/test functions\n",
    "# Adapted from the pytorch quickstart guide\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimiser:torch.optim.Optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0 # For graphing the change in loss over time\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute preds and loss for batch\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds,y)\n",
    "\n",
    "        #Backprop\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "        train_loss += loss * len(X) #Weight the average to account for the last batch\n",
    "\n",
    "        #Progress tracking\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= size\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    # we will compute a confusion matrix to allow for calculation of\n",
    "    confusion_matrix = torch.zeros(NUM_CLASSES, NUM_CLASSES)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y, = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            test_loss += loss_fn(preds, y).item()\n",
    "            pred_classes = preds.argmax(1)\n",
    "            confusion_matrix += compute_confusion_matrix(pred_classes, y, NUM_CLASSES)\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    class_precision_recall, avg_precision, avg_recall = compute_precision_recall(confusion_matrix, print_output=True)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return [test_loss, correct, confusion_matrix, class_precision_recall,avg_precision, avg_recall]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Run the test a training loops for 5 epochs\n",
    "\n",
    "def train_with_early_stopping(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval=2, min_epochs=2):\n",
    "\n",
    "    stop, epoch = False, 1\n",
    "    train_losses = []\n",
    "    test_results = []\n",
    "    test_losses = []\n",
    "    while not stop:\n",
    "        print(f\"---------- Epoch {epoch} ---------\")\n",
    "        train_losses.append(train(train_dataloader, model, loss_fn, optimiser)) #Train\n",
    "        test_result = test(test_dataloader, model, loss_fn) #Test\n",
    "        # Compile test results\n",
    "        test_results.append(test_result)\n",
    "        test_losses.append(test_result[0])\n",
    "\n",
    "        #Save model\n",
    "        if epoch % save_interval == 0:\n",
    "            filename = f\"{model.name}_{epoch}.pth\"\n",
    "            torch.save(model.state_dict(), filename)\n",
    "\n",
    "        #check if current test loss is worse than last 2 epochs\n",
    "        if epoch > min_epochs and epoch > 2:\n",
    "            check1 = test_losses[-1] > test_losses[-2]\n",
    "            check2 = test_losses[-1] > test_losses[-3]\n",
    "            if check1 and check2:\n",
    "                stop = True\n",
    "                print(f\"Early stopping triggered at epoch {epoch}. Done!\")\n",
    "        # next epoch\n",
    "        epoch += 1\n",
    "    return train_losses, test_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(fc_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, fc_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Plot the results of a training session using pyplotlib\n",
    "# Currently plotting the following:\n",
    "#  - Training loss vs Test loss\n",
    "#  - Average precision, recall, and accuracy (test set)\n",
    "#  - Precision per class\n",
    "#  - Recall per class\n",
    "def plot_results(train_losses, test_results):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "    # Plot TrainLoss on ax1\n",
    "    xs, ys = [], []\n",
    "    for epoch, loss in enumerate(train_losses):\n",
    "        xs.append(epoch+1)\n",
    "        ys.append(loss)\n",
    "\n",
    "    ax1.plot(xs, ys, label=\"Train Loss\")\n",
    "\n",
    "    # Plot TestLoss on ax1\n",
    "    test_losses = [i[0] for i in test_results]\n",
    "    xs, ys = [], []\n",
    "    for epoch, loss in enumerate(test_losses):\n",
    "        xs.append(epoch+1)\n",
    "        ys.append(loss)\n",
    "    ax1.plot(xs, ys, label=\"Test Loss\")\n",
    "\n",
    "    # Setup ax1 to show test vs train loss\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(\"Test vs. Train Loss\")\n",
    "\n",
    "    # Plot avg precision (ax2)\n",
    "    ys = [i[4]*100 for i in test_results]\n",
    "    ax2.plot(xs, ys, label='Precision')\n",
    "\n",
    "    # Plot avg recall (ax2)\n",
    "    ys = [i[5]*100 for i in test_results]\n",
    "    ax2.plot(xs, ys, label='Recall')\n",
    "\n",
    "    # Setup ax2\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Percentage\")\n",
    "    ax2.set_title(\"Average Precision/Recall\")\n",
    "\n",
    "    # Show per class precision/recall (ax3/4)\n",
    "    precision_recalls = [i[3] for i in test_results]\n",
    "    xs = [i+1 for i in range(len(precision_recalls))]\n",
    "    for idx, class_name in enumerate(CLASS_NAMES):\n",
    "        ys_p, ys_r = [],[]\n",
    "        for p_r in precision_recalls:\n",
    "            ys_p.append(p_r[idx, 0]*100)\n",
    "            ys_r.append(p_r[idx, 1]*100)\n",
    "        # Precision on ax3, recall on ax4\n",
    "        ax3.plot(xs, ys_p, label=class_name)\n",
    "        ax4.plot(xs, ys_r, label=class_name)\n",
    "    # Setup ax3,4\n",
    "    ax3.legend()\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Percentage\")\n",
    "    ax3.set_title(\"Per Class Precision\")\n",
    "    ax4.legend()\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(\"Percentage\")\n",
    "    ax4.set_title(\"Per Class Recall\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(train_losses, test_results)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "The first network is based on AlexNet, but scaled down to a 32x32 input"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "JoeNet(\n  (conv1): Conv2d(3, 6, kernel_size=(7, 7), stride=(2, 2))\n  (pool1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n  (pool2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (conv4): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (conv5): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=400, out_features=256, bias=True)\n  (dropout1): Dropout(p=0.2, inplace=False)\n  (fc2): Linear(in_features=256, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class JoeNet(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(JoeNet, self).__init__()\n",
    "        self.name = config['name']\n",
    "        self.batch_norm = config['batch_norm']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 96, 7, 2) #32x32 -> 13x13\n",
    "        if self.batch_norm: self.conv1_bn = nn.BatchNorm2d(96)\n",
    "        self.pool1 = nn.MaxPool2d(3, 1, padding=1) #13x13 -> 13x13\n",
    "        self.conv2 = nn.Conv2d(96,256,5,padding=\"same\") #13x13 -> 13x13\n",
    "        if self.batch_norm: self.conv2_bn = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(3, 1) #13x13 -> 12x12\n",
    "        self.conv3 = nn.Conv2d(256, 384, 3, padding=\"same\")\n",
    "        if self.batch_norm: self.conv3_bn = nn.BatchNorm2d(384)\n",
    "        self.conv4 = nn.Conv2d(384, 384, 3, padding=\"same\")\n",
    "        if self.batch_norm: self.conv4_bn = nn.BatchNorm2d(384)\n",
    "        self.conv5 = nn.Conv2d(384, 256, 3, padding=\"same\")\n",
    "        if self.batch_norm: self.conv5_bn = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=2)\n",
    "        self.fc1 = nn.Linear(6400, 4096)\n",
    "        if self.batch_norm: self.fc1_bn = nn.BatchNorm1d(4096)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        if self.batch_norm: self.fc2_bn = nn.BatchNorm1d(4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm: x = self.conv1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm: x = self.conv2_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.batch_norm: x = self.conv3_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        if self.batch_norm: x = self.conv4_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        if self.batch_norm: x = self.conv5_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        if self.batch_norm: x = self.fc1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.batch_norm: x = self.fc2_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "jn_config = {\n",
    "    'name' : 'JoeNet_test',\n",
    "    'batch_norm' : False\n",
    "}\n",
    "\n",
    "j_net = JoeNet(jn_config)\n",
    "j_net.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ---------\n",
      "loss: 2.303792  [    0/50000]\n",
      "loss: 2.305553  [ 6400/50000]\n",
      "loss: 2.300005  [12800/50000]\n",
      "loss: 2.311383  [19200/50000]\n",
      "loss: 2.300963  [25600/50000]\n",
      "loss: 2.302633  [32000/50000]\n",
      "loss: 2.306851  [38400/50000]\n",
      "loss: 2.299073  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.01%, Recall: 99.20\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: 8.14%, Recall: 0.70\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 9.99%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.303010 \n",
      "\n",
      "---------- Epoch 2 ---------\n",
      "loss: 2.305445  [    0/50000]\n",
      "loss: 2.302109  [ 6400/50000]\n",
      "loss: 2.299140  [12800/50000]\n",
      "loss: 2.308956  [19200/50000]\n",
      "loss: 2.299933  [25600/50000]\n",
      "loss: 2.301948  [32000/50000]\n",
      "loss: 2.308444  [38400/50000]\n",
      "loss: 2.298126  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.00%, Recall: 100.00\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: 0.00%, Recall: 0.00\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 10.00%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302928 \n",
      "\n",
      "---------- Epoch 3 ---------\n",
      "loss: 2.303555  [    0/50000]\n",
      "loss: 2.304573  [ 6400/50000]\n",
      "loss: 2.301187  [12800/50000]\n",
      "loss: 2.308181  [19200/50000]\n",
      "loss: 2.298380  [25600/50000]\n",
      "loss: 2.301726  [32000/50000]\n",
      "loss: 2.310208  [38400/50000]\n",
      "loss: 2.301746  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.00%, Recall: 100.00\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: nan%, Recall: 0.00\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 10.00%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302862 \n",
      "\n",
      "---------- Epoch 4 ---------\n",
      "loss: 2.303605  [    0/50000]\n",
      "loss: 2.302273  [ 6400/50000]\n",
      "loss: 2.300882  [12800/50000]\n",
      "loss: 2.305667  [19200/50000]\n",
      "loss: 2.300146  [25600/50000]\n",
      "loss: 2.303330  [32000/50000]\n",
      "loss: 2.306032  [38400/50000]\n",
      "loss: 2.298324  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.00%, Recall: 100.00\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: nan%, Recall: 0.00\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 10.00%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302808 \n",
      "\n",
      "---------- Epoch 5 ---------\n",
      "loss: 2.302212  [    0/50000]\n",
      "loss: 2.303984  [ 6400/50000]\n",
      "loss: 2.300748  [12800/50000]\n",
      "loss: 2.306323  [19200/50000]\n",
      "loss: 2.301301  [25600/50000]\n",
      "loss: 2.305068  [32000/50000]\n",
      "loss: 2.307743  [38400/50000]\n",
      "loss: 2.300608  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.00%, Recall: 100.00\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: nan%, Recall: 0.00\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 10.00%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302764 \n",
      "\n",
      "---------- Epoch 6 ---------\n",
      "loss: 2.303557  [    0/50000]\n",
      "loss: 2.301788  [ 6400/50000]\n",
      "loss: 2.302133  [12800/50000]\n",
      "loss: 2.308433  [19200/50000]\n",
      "loss: 2.301447  [25600/50000]\n",
      "loss: 2.305058  [32000/50000]\n",
      "loss: 2.306347  [38400/50000]\n",
      "loss: 2.301238  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.00%, Recall: 100.00\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: nan%, Recall: 0.00\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 10.00%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302729 \n",
      "\n",
      "---------- Epoch 7 ---------\n",
      "loss: 2.303661  [    0/50000]\n",
      "loss: 2.303286  [ 6400/50000]\n",
      "loss: 2.302073  [12800/50000]\n",
      "loss: 2.306920  [19200/50000]\n",
      "loss: 2.302865  [25600/50000]\n",
      "loss: 2.302878  [32000/50000]\n",
      "loss: 2.307507  [38400/50000]\n",
      "loss: 2.300325  [44800/50000]\n",
      "Class: 0, Precision: nan%, Recall: 0.00\n",
      "Class: 1, Precision: 10.00%, Recall: 100.00\n",
      "Class: 2, Precision: nan%, Recall: 0.00\n",
      "Class: 3, Precision: nan%, Recall: 0.00\n",
      "Class: 4, Precision: nan%, Recall: 0.00\n",
      "Class: 5, Precision: nan%, Recall: 0.00\n",
      "Class: 6, Precision: nan%, Recall: 0.00\n",
      "Class: 7, Precision: nan%, Recall: 0.00\n",
      "Class: 8, Precision: nan%, Recall: 0.00\n",
      "Class: 9, Precision: nan%, Recall: 0.00\n",
      "Average Precision: nan%, Average Recall: 10.00%\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302700 \n",
      "\n",
      "---------- Epoch 8 ---------\n",
      "loss: 2.303735  [    0/50000]\n",
      "loss: 2.304754  [ 6400/50000]\n",
      "loss: 2.301802  [12800/50000]\n",
      "loss: 2.304480  [19200/50000]\n",
      "loss: 2.301352  [25600/50000]\n",
      "loss: 2.301511  [32000/50000]\n",
      "loss: 2.305701  [38400/50000]\n",
      "loss: 2.300411  [44800/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(j_net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#     for batch, (X, y) in enumerate(test_dataloader):\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#         preds =j_net(X)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#test_results = test(test_dataloader, j_net, loss_fn)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#print(test_results[2])\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m train_losses, test_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_with_early_stopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mtrain_with_early_stopping\u001B[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval, min_epochs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stop:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---------- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#Train\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     test_result \u001B[38;5;241m=\u001B[39m test(test_dataloader, model, loss_fn) \u001B[38;5;66;03m#Test\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Compile test results\u001B[39;00m\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataloader, model, loss_fn, optimiser)\u001B[0m\n\u001B[1;32m     30\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# For graphing the change in loss over time\u001B[39;00m\n\u001B[1;32m     31\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (X, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Compute preds and loss for batch\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     preds \u001B[38;5;241m=\u001B[39m model(X)\n\u001B[1;32m     35\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(preds,y)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torchvision/datasets/cifar.py:118\u001B[0m, in \u001B[0;36mCIFAR10.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    115\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 118\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    121\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torchvision/transforms/transforms.py:135\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torchvision/transforms/functional.py:155\u001B[0m, in \u001B[0;36mto_tensor\u001B[0;34m(pic)\u001B[0m\n\u001B[1;32m    153\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mpermute((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mByteTensor):\n\u001B[0;32m--> 155\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_float_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdiv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(j_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch, (X, y) in enumerate(test_dataloader):\n",
    "#         preds =j_net(X)\n",
    "#         classes = preds.argmax(1)\n",
    "#         print(classes)\n",
    "#         if batch > 3: break\n",
    "\n",
    "#test_results = test(test_dataloader, j_net, loss_fn)\n",
    "#print(test_results[2])\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, j_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This second CNN is based on VGG16, but one max pooling layer has changed to ensure the\n",
    "size is not reduced below 3x3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16ish(\n",
      "  (conv_stack): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Identity()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): Identity()\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (8): Identity()\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (11): Identity()\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (15): Identity()\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (18): Identity()\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (21): Identity()\n",
      "    (22): ReLU()\n",
      "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (24): Identity()\n",
      "    (25): ReLU()\n",
      "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (28): Identity()\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (31): Identity()\n",
      "    (32): ReLU()\n",
      "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (34): Identity()\n",
      "    (35): ReLU()\n",
      "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (37): Identity()\n",
      "    (38): ReLU()\n",
      "    (39): MaxPool2d(kernel_size=2, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (41): Identity()\n",
      "    (42): ReLU()\n",
      "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (44): Identity()\n",
      "    (45): ReLU()\n",
      "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (47): Identity()\n",
      "    (48): ReLU()\n",
      "    (49): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_stack): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (1): Identity()\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): Identity()\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VGG16ish(nn.Module):\n",
    "    def __init__(self, config:dict):\n",
    "        super(VGG16ish, self).__init__()\n",
    "\n",
    "        self.name = config['name']\n",
    "        self.batch_norm = config['batch_norm']\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            # Conv stack 1 - 2 layers 64 features\n",
    "            nn.Conv2d(3,64,3,padding='same'), # 32x32\n",
    "            nn.BatchNorm2d(64) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,3,padding='same'), # 32x32\n",
    "            nn.BatchNorm2d(64) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 32x32 -> 16x16\n",
    "            # Conv stack 2 - 2 layers 128 features\n",
    "            nn.Conv2d(64,128,3,padding='same'), # 16x16\n",
    "            nn.BatchNorm2d(128) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,3,padding='same'), # 16x16\n",
    "            nn.BatchNorm2d(128) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 16x16 -> 8x8\n",
    "            # Conv stack 3 - 3 layers 256 features\n",
    "            nn.Conv2d(128,256,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(256) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(256) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(256) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256,3,padding='same'), # 8x8\n",
    "            nn.BatchNorm2d(256) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), #  8x8 -> 4x4\n",
    "            # Conv stack 4 - 3 layers 512 features\n",
    "            nn.Conv2d(256,512,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1, padding=1), #  4x4 -> 4x4\n",
    "            # Conv stack 5 - 3 layers 512 features\n",
    "            nn.Conv2d(512,512,3,padding='same'),# 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,512,3,padding='same'), # 4x4\n",
    "            nn.BatchNorm2d(512) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) #2x2x512\n",
    "        )\n",
    "\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.BatchNorm1d(4096) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.BatchNorm1d(4096) if self.batch_norm else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_stack(x)\n",
    "        return x\n",
    "\n",
    "vgg_net_config = {\n",
    "    'name' : 'vgg_test',\n",
    "    'batch_norm' : False\n",
    "}\n",
    "\n",
    "vgg_net = VGG16ish(vgg_net_config)\n",
    "print(vgg_net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Epoch 1 ---------\n",
      "loss: 2.302320  [    0/50000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m      2\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(vgg_net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, momentum\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m train_losses, test_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_with_early_stopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvgg_net\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mtrain_with_early_stopping\u001B[0;34m(train_dataloader, test_dataloader, model, loss_fn, optimiser, save_interval, min_epochs)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stop:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m---------- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m#Train\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     test_result \u001B[38;5;241m=\u001B[39m test(test_dataloader, model, loss_fn) \u001B[38;5;66;03m#Test\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Compile test results\u001B[39;00m\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataloader, model, loss_fn, optimiser)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#Backprop\u001B[39;00m\n\u001B[1;32m     38\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/coc102/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(vgg_net.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "train_losses, test_results = train_with_early_stopping(train_dataloader, test_dataloader, vgg_net, loss_fn, optimiser)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}